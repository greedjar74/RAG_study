import streamlit as st
import os
import sys
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from pypdf import PdfReader

# ğŸ” ì›¹ ê²€ìƒ‰ ë° í¬ë¡¤ë§
def search_web(query, num_results=3, api_key=None):
    headers = {"X-API-KEY": api_key, "Content-Type": "application/json"} # API í˜¸ì¶œ ì‹œ í•„ìš”í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” HTTP í—¤ë”
    payload = {"q": query, "num": num_results} # ê²€ìƒ‰ ìš”ì²­ ë°ì´í„° -> ê²€ìƒ‰ì–´, ìš”ì²­ ê²°ê³¼ ê°œìˆ˜
    response = requests.post("https://google.serper.dev/search", json=payload, headers=headers) # post ìš”ì²­
    results = response.json().get("organic", []) # ì‘ë‹µì„ json í˜•ì‹ìœ¼ë¡œ íŒŒì‹±, organic: ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì„ ì˜ë¯¸
    return [item["link"] for item in results] # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ linkë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜ -> ì œëª©, ìš”ì•½ ì„¤ëª…, ì›¹ì‚¬ì´íŠ¸ ì•„ì´ì½˜ ë“±ì€ í•„ìš” ì—†ê¸°ì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

# urlì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=5) # urlìœ¼ë¡œ ì´ë™ ìš”ì²­
        soup = BeautifulSoup(response.text, "html.parser") # html í…ìŠ¤íŠ¸ë¥¼ BeautifulSoup ê°ì²´ë¡œ íŒŒì‹±
        for tag in soup(["script", "style"]): tag.decompose() # <script>, <style> íƒœê·¸ ì œê±° -> ë²„íŠ¼ ë™ì‘ ì •ì˜, ê¸€ê¼´ ì •ì˜ ë“± ì˜ë¯¸ê°€ ì—†ëŠ” ì •ë³´
        return soup.get_text(separator="\n").strip() # htmlì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
    except Exception:
        return ""

# ğŸ§  í…ìŠ¤íŠ¸ ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# ğŸ“˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (pypdf ì‚¬ìš©)
def extract_text_from_pdf(file):
    text = ""
    try:
        reader = PdfReader(file) # pdfíŒŒì¼ ë¡œë“œ
        for page in reader.pages: 
            text += page.extract_text() or "" # ê° í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ text ë³€ìˆ˜ì— ì¶”ê°€
        return text
    except Exception:
        return ""

# ğŸ“š ë¬¸ì„œ ë²¡í„°í™” - ì›¹ ê¸°ë°˜
def get_search_docs(query, embedding_model, serper_key, k=5):
    urls = search_web(query, num_results=3, api_key=serper_key) # ê´€ë ¨ url í¬ë¡¤ë§
    docs = []
    for url in urls:
        text = extract_text_from_url(url) # ì›¹ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if text:
            chunks = text_splitter.split_text(text) # ë¬¸ì„œë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í• 
            docs.extend([Document(page_content=chunk, metadata={"source": url}) for chunk in chunks]) # docsì— ê° chunkë¥¼ ì¶”ê°€
    if docs:
        vectorstore = FAISS.from_documents(docs, embedding_model) # chunkë‹¨ìœ„ë¡œ ë‚˜ëˆ ì§„ ë¬¸ì„œ ë°ì´í„°ë¥¼ FAISSì— ì €ì¥
        retriever = vectorstore.as_retriever(search_kwargs={"k": k}) # retriever êµ¬ì„±
        return retriever, docs
    return None, []

# ğŸ“š ë¬¸ì„œ ë²¡í„°í™” - PDF ê¸°ë°˜
def get_pdf_docs(file, embedding_model, k=5):
    text = extract_text_from_pdf(file) # pdfì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    if not text:
        return None, []

    chunks = text_splitter.split_text(text) # í…ìŠ¤íŠ¸ë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í• 
    docs = [Document(page_content=chunk, metadata={"source": "ì—…ë¡œë“œëœ PDF"}) for chunk in chunks] # docsì— ê° chunkë¥¼ ì¶”ê°€
    if docs:
        vectorstore = FAISS.from_documents(docs, embedding_model) # chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì§„ ë¬¸ì„œ ë°ì´í„°ë¥¼ FAISSì— ì €ì¥
        retriever = vectorstore.as_retriever(search_kwargs={"k": k}) # retriever êµ¬ì„±
        return retriever, docs
    return None, []

# ğŸ§  RAG ì²˜ë¦¬
def run_RAG(user_input, chat_history, mode, embedding_model, serper_key=None, pdf_file=None):
    # queryì— ëŒ€í•œ retriever ìƒì„±
    if mode == "ì›¹ ê²€ìƒ‰":
        retriever, all_docs = get_search_docs(user_input, embedding_model, serper_key)
    elif mode == "PDF íŒŒì¼":
        retriever, all_docs = get_pdf_docs(pdf_file, embedding_model)
    else:
        return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œì…ë‹ˆë‹¤.", [], []

    if not retriever:
        return "ê²€ìƒ‰ëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", [], []

    docs = retriever.invoke(user_input) # ì‚¬ìš©ì ì…ë ¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ íƒìƒ‰
    context = "\n\n".join(doc.page_content for doc in docs) # ì°¾ìŒ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ í˜•íƒœë¡œ ë§Œë“ ë‹¤.
    conversation = "\n".join(chat_history + [f"user: {user_input}"]) # ëŒ€í™” ë‚´ìš©ì„ ì¶”ê°€

    # prompt template ìƒì„±
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""
ë„ˆëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì•¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸ê³¼ ëŒ€í™” ë‚´ìš©]
{question}

[ë‹µë³€]
"""
    )

    prompt_text = prompt_template.format(context=context, question=conversation) # prompt í˜•ì‹ì— ë§ì¶° prompt ì™„ì„±
    llm = ChatOpenAI(model="gpt-4.1-mini") # llm ëª¨ë¸ ì„¤ì •
    response = llm.invoke(prompt_text) # llm ë‹µë³€ ìƒì„±
    return response.content, docs, [doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in docs] # ê²°ê³¼ ë°˜í™˜

# ğŸ–¥ï¸ Streamlit UI
def rag_chatbot():
    st.title("ğŸ” ì‹¤ì‹œê°„ ê²€ìƒ‰ & PDF ê¸°ë°˜ RAG Chatbot")

    st.sidebar.header("ğŸ”§ ëª¨ë“œ ì„ íƒ")
    mode = st.sidebar.radio("ì§ˆë¬¸ì— ì°¸ê³ í•  ì†ŒìŠ¤ ì„ íƒ", ["ì›¹ ê²€ìƒ‰", "PDF íŒŒì¼"])

    st.sidebar.header("ğŸ” API ì„¤ì •")
    openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
    serper_key = None
    pdf_file = None

    if mode == "ì›¹ ê²€ìƒ‰":
        serper_key = st.sidebar.text_input("Serper.dev API Key", type="password")
    elif mode == "PDF íŒŒì¼":
        pdf_file = st.sidebar.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

    if not openai_key or (mode == "ì›¹ ê²€ìƒ‰" and not serper_key) or (mode == "PDF íŒŒì¼" and not pdf_file):
        st.warning("í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ì…ë ¥í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop()

    os.environ["OPENAI_API_KEY"] = openai_key
    sys.stdout.reconfigure(encoding="utf-8")

    # ëŒ€í™” ë‚´ì—­ì´ ìˆëŠ”ì§€ í™•ì¸ -> ì—†ëŠ” ê²½ìš° chat_hisory ìƒì„±
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                embeddings = OpenAIEmbeddings(model="text-embedding-3-small") # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
                # RAG ì‹¤í–‰ : ì›¹ or ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ -> ì„ë² ë”© -> ë²¡í„° DBì— ì €ì¥ -> ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”© -> ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ íƒìƒ‰ -> prompt ì™„ì„± -> llm ë‹µë³€ ìƒì„±
                response_text, docs_used, sources = run_RAG(
                    user_input, # ì‚¬ìš©ì ì…ë ¥
                    [f'{m["role"]}: {m["content"]}' for m in st.session_state.chat_history], # ëŒ€í™”ë‚´ì—­ ì „ë‹¬ {llm or ì‚¬ëŒ: ë‚´ìš©} í˜•ì‹
                    mode, # web search or pdf ëª¨ë“œ ì„¤ì •
                    embeddings, # ì„ë² ë”© ëª¨ë¸
                    serper_key=serper_key,
                    pdf_file=pdf_file,
                )

                with st.chat_message("assistant"):
                    st.markdown(response_text)
                    with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ"):
                        for i, (doc, src) in enumerate(zip(docs_used, sources), 1):
                            preview = doc.page_content[:300].replace("\n", " ")
                            st.markdown(f"**[{i}]** [{src}]({src})\n\n{preview}...")

                # chat historyì— ì‚¬ìš©ì ì…ë ¥, LLM ë‹µë³€ ì¶”ê°€
                st.session_state.chat_history.append({"role": "user", "content": user_input})
                st.session_state.chat_history.append({"role": "assistant", "content": response_text})

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if st.button("ğŸ” ëŒ€í™” ë¦¬ì…‹"):
        st.session_state.chat_history = []
        st.rerun()