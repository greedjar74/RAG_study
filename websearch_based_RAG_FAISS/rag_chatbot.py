import streamlit as st
import os
import sys
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from pypdf import PdfReader

st.set_page_config(layout="centered")

# ì›¹ ê²€ìƒ‰
def search_web(query, num_results=3, api_key=None):
    headers = {"X-API-KEY": api_key, "Content-Type": "application/json"}
    payload = {"q": query, "num": num_results}
    response = requests.post("https://google.serper.dev/search", json=payload, headers=headers)
    return [item["link"] for item in response.json().get("organic", [])]

# URL í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style"]): tag.decompose()
        return soup.get_text(separator="\n").strip()
    except Exception:
        return ""

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(file):
    text = ""
    try:
        reader = PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception:
        return ""

# í…ìŠ¤íŠ¸ ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# ë¬¸ì„œ ê²°í•©
def get_combined_docs(company_name, pdf_file, embedding_model, serper_key, k=10):
    urls = search_web(f"{company_name} ë©´ì ‘ í›„ê¸° ì§ˆë¬¸ í•©ê²© íŒ", num_results=3, api_key=serper_key)
    docs = []
    for url in urls:
        text = extract_text_from_url(url)
        if text:
            chunks = text_splitter.split_text(text)
            docs.extend([Document(page_content=chunk, metadata={"source": url}) for chunk in chunks])

    pdf_text = extract_text_from_pdf(pdf_file)
    if pdf_text:
        chunks = text_splitter.split_text(pdf_text)
        docs.extend([Document(page_content=chunk, metadata={"source": "ìê¸°ì†Œê°œì„œ"}) for chunk in chunks])

    if docs:
        vectorstore = FAISS.from_documents(docs, embedding_model)
        retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        return retriever, docs
    return None, []

# ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
def generate_interview_questions(company_name, pdf_file, embedding_model, serper_key):
    retriever, all_docs = get_combined_docs(company_name, pdf_file, embedding_model, serper_key)
    if not retriever:
        return "âŒ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", [], []

    query = f"{company_name} ê¸°ì—… ë©´ì ‘ì„ ì¤€ë¹„ ì¤‘ì´ì•¼. ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì¤˜."
    docs = retriever.invoke(query)
    context = "\n\n".join(doc.page_content for doc in docs)

    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""
ë„ˆëŠ” ì¸ì‚¬ë‹´ë‹¹ìì²˜ëŸ¼ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ëŠ” AIì•¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ìš”ì²­]
{question}

[ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸]
"""
    )

    prompt_text = prompt_template.format(context=context, question=query)
    llm = ChatOpenAI(model="gpt-4.1-mini")
    response = llm.invoke(prompt_text)
    return response.content, docs, [doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in docs]

# Streamlit UI
def rag_chatbot():
    st.title("ìŠ¤ë¬´ë””")

    for key in ["questions", "current_q", "user_answers", "docs_used", "sources"]:
        if key not in st.session_state:
            st.session_state[key] = []

    # API í‚¤ ì…ë ¥
    st.sidebar.header("ğŸ” API Key ì„¤ì •")
    openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
    serper_key = st.sidebar.text_input("Serper.dev API Key", type="password")

    # ê¸°ë³¸ ì •ë³´ ì…ë ¥
    company_name = st.text_input("1ï¸âƒ£ ì§€ì›í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì‚¼ì„±ì „ì, ì¹´ì¹´ì˜¤")
    pdf_file = st.file_uploader("2ï¸âƒ£ ìê¸°ì†Œê°œì„œ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

    if not openai_key or not serper_key or not company_name or not pdf_file:
        st.warning("ëª¨ë“  í•­ëª©ì„ ì…ë ¥í•˜ê³  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop()

    os.environ["OPENAI_API_KEY"] = openai_key
    sys.stdout.reconfigure(encoding="utf-8")

    # ì§ˆë¬¸ ìƒì„± ë²„íŠ¼
    if st.button("ğŸš€ ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ìƒì„± ì‹œì‘"):
        with st.spinner("ì§ˆë¬¸ ìƒì„± ì¤‘..."):
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            response_text, docs_used, sources = generate_interview_questions(company_name, pdf_file, embeddings, serper_key)

            questions = [q.strip("-â€¢â— ").strip() for q in response_text.strip().split("\n") if q.strip()]
            st.session_state.questions = questions
            st.session_state.current_q = 0
            st.session_state.user_answers = []
            st.session_state.docs_used = docs_used
            st.session_state.sources = sources

            st.success("ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ! ì•„ë˜ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.")
            st.rerun()
    
    # ì „ì²´ ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°
    if st.session_state.questions:
        with st.expander("ğŸ“‹ ìƒì„±ëœ ì „ì²´ ì§ˆë¬¸ ëª©ë¡ ë³´ê¸°"):
            for idx, question in enumerate(st.session_state.questions, 1):
                st.markdown(f"**{idx}. {question}**")

    # ì§ˆë¬¸/ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
    if st.session_state.questions:
        curr_idx = st.session_state.current_q
        if curr_idx < len(st.session_state.questions):
            curr_q = st.session_state.questions[curr_idx]
            st.subheader(f"ğŸ“ ì§ˆë¬¸ {curr_idx + 1}")
            st.markdown(f"**{curr_q}**")

            answer = st.text_area("âœï¸ ë‹¹ì‹ ì˜ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”", key=f"answer_{curr_idx}")

            if st.button("â¡ï¸ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ"):
                if answer.strip():
                    st.session_state.user_answers.append(answer.strip())
                    st.session_state.current_q += 1
                    st.rerun()
                else:
                    st.warning("ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

            # âœ… ì´ì „ ì§ˆë¬¸ ë° ë‹µë³€ ì¶œë ¥
            if st.session_state.user_answers:
                st.markdown("---")
                st.markdown("### ğŸ“Œ ì´ì „ ì§ˆë¬¸ ë° ë‹µë³€")
                for i, (q, a) in enumerate(zip(
                    st.session_state.questions[:curr_idx],
                    st.session_state.user_answers
                ), 1):
                    st.markdown(f"**Q{i}: {q}**")
                    st.markdown(f"ğŸ—£ **ë‹µë³€:** {a}")
                    st.markdown("---")

        else:
            st.success("ğŸ‰ ëª¨ë“  ì§ˆë¬¸ì— ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤!")

            for i, (q, a) in enumerate(zip(
                st.session_state.questions,
                st.session_state.user_answers
            ), 1):
                st.markdown(f"---\n**Q{i}: {q}**")
                st.markdown(f"ğŸ—£ **ë‹µë³€:** {a}")

            with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                for i, (doc, src) in enumerate(zip(st.session_state.docs_used, st.session_state.sources), 1):
                    preview = doc.page_content[:300].replace("\n", " ")
                    st.markdown(f"**[{i}]** [{src}]({src})\n\n{preview}...")