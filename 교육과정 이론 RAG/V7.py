# V7 - Chroma DBì— ëŒ€í™” ë‚´ìš© ì €ì¥
import os
import sys
import sqlite3
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# í™˜ê²½ ì„¤ì •
os.environ["OPENAI_API_KEY"] = ""  # ğŸ” OpenAI API í‚¤ ì…ë ¥
sys.stdout.reconfigure(encoding='utf-8')

# PDF ë¡œë”© ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
reader = PdfReader("summary of curriculum theory.pdf")
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)
docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]

# DB ê²½ë¡œ
db_path = "./curriculum_theory_DB"

# DBê°€ ìˆëŠ” ê²½ìš° -> ë¡œë“œ
if os.path.exists(db_path):
    vectorstore = Chroma(
        persist_directory=db_path,
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    )

# DBê°€ ì—†ëŠ” ê²½ìš° -> ìƒì„±
else:
    vectorstore = Chroma.from_documents(
        docs,
        OpenAIEmbeddings(model="text-embedding-3-small"),
        persist_directory=db_path
    )
    vectorstore.persist()

# DB retriever ì„¤ì •
retriever = vectorstore.as_retriever()

# LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4.1-mini")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë„ì›€ì„ ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— êµ¬ì²´ì ìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ëŒ€í™” ë‚´ìš© ë° ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""
)

# ë¬¸ì„œ í¬ë§·
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ğŸ”„ ëŒ€í™” ë‚´ìš©ì„ Chroma DBì— ì €ì¥
def store_chat_to_chroma(user_input, assistant_reply):
    conversation_text = f"user: {user_input}\nassistant: {assistant_reply}"
    doc = Document(page_content=conversation_text)
    vectorstore.add_documents([doc])
    vectorstore.persist()

# RAG ì‹¤í–‰ í•¨ìˆ˜
def run_RAG(user_input, chat_history):
    docs = retriever.invoke(user_input)
    context = format_docs(docs)
    conversation = '\n'.join(chat_history + [f'user: {user_input}'])
    prompt_text = prompt.format(context=context, question=conversation)
    response = llm.invoke(prompt_text)
    return response.content, docs

# ğŸ” ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° (ë©”ëª¨ë¦¬ ê¸°ì¤€, DBì— ì €ì¥ X)
chat_history = []

# ëŒ€í™” ë£¨í”„
print("RAG ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)\n")
while True:
    user_input = input("ğŸ‘¤ ì§ˆë¬¸: ").strip()
    if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
        print("ğŸ›‘ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    try:
        response_text, source_docs = run_RAG(user_input, chat_history)

        print("\nğŸ¤– ë‹µë³€:")
        for sentence in response_text.split('. '):
            print(sentence.strip())

        print('\nğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©:')
        for i, doc in enumerate(source_docs, 1):
            print(f'\n[{i}] {doc.page_content.strip()[:300]}...')

        # âœ… Chroma DBì— ëŒ€í™” ì €ì¥
        store_chat_to_chroma(user_input, response_text)

        # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥ (ë§¥ë½ ìœ ì§€)
        chat_history.append(f"user: {user_input}")
        chat_history.append(f"assistant: {response_text}")

    except Exception as e:
        print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)
