'''
# V5
- ëŒ€í™”í˜•
- ê¸°ì¡´ ëŒ€í™” ê¸°ì–µ
- DB ì˜êµ¬ ì €ì¥
- prompt í˜•íƒœ ì‚¬ìš©ìí™”
- rag_chain ì‚¬ìš© -> ë‹¨ìˆœí•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì§€ë§Œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê³ ë ¤ ëª»í•¨
'''

from langchain import hub
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
import sys
import os

# í™˜ê²½ ì„¤ì •
os.environ["OPENAI_API_KEY"] = ""  # ì‹¤ì œ í‚¤ë¡œ êµì²´í•˜ì„¸ìš”
sys.stdout.reconfigure(encoding='utf-8')

# PDF ë¶ˆëŸ¬ì˜¤ê¸° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
reader = PdfReader("ì»¤ë¦¬í˜ëŸ¼ ì œì‘ ì°¸ê³ ìë£Œ.pdf")
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]

# ì˜êµ¬ ë²¡í„° DB ìƒì„± ë° ì €ì¥
db_path = "./êµìœ¡ê³¼ì •_ì´ë¡ _DB"

if os.path.exists(db_path):
    vectorstore = Chroma(
        persist_directory=db_path,
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    )
else :
    vectorstore = Chroma.from_documents(
        docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=db_path
    )
    vectorstore.persist()

# retriever ì„¤ì •
retriever = vectorstore.as_retriever()

# LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì„±ì‹¤í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""
)

# RAG ì²´ì¸ ì •ì˜
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()} # RunnablePassthrough : ë‹¨ìˆœí•˜ê²Œ ì‚¬ìš©ì ì…ë ¥ë§Œ ì „ë‹¬
    | prompt
    | llm
    | StrOutputParser()
)

chat_history = []

# ëŒ€í™” ë£¨í”„
print("RAG ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)\n")
while True:
    user_input = input("ğŸ‘¤ ì§ˆë¬¸: ").strip()
    if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
        print("ğŸ›‘ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    full_prompt = '\n'.join(chat_history + [f"user: {user_input}"])

    try:
        response = rag_chain.invoke(full_prompt)
        print("\nğŸ¤– ë‹µë³€:")
        for sentence in response.split('. '):
            print(sentence.strip())
        print()  # ì¤„ë°”ê¿ˆ
    except Exception as e:
        print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)
    
    chat_history.append(f"user: {user_input}")
    chat_history.append(f"assistant: {response}")

    