'''
# nowcoding ë§ì¶¤í˜• RAG êµ¬ì¶•
- gpt api ê¸°ë°˜ (gpt-4.1-mini)
- chroma db ì‚¬ìš©
- langchain í™œìš©
- ì›í•˜ëŠ” ê²½ìš° ì‚¬ìš©ì ì…ë ¥ì„ DBì— ì €ì¥í•  ìˆ˜ ìˆë‹¤.
- DB ë‚´ìš© í™•ì¸ ê°€ëŠ¥
- ìƒë‹¹íˆ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì¸ë‹¤.
'''

import streamlit as st
import os
import sys
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate

def nowcoding_RAG():
    st.title("RAG ì‹¤ìŠµ")

    # ğŸ”‘ OpenAI API ì„¤ì •
    st.sidebar.header("ğŸ” API ì„¤ì •")
    api_key_input = st.sidebar.text_input("OpenAI API Key", type="password")
    if not api_key_input:
        st.warning("ì‚¬ì´ë“œë°”ì— API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    os.environ["OPENAI_API_KEY"] = api_key_input
    sys.stdout.reconfigure(encoding="utf-8")

    # PDF ë¬¸ì„œ ë¡œë”©
    pdf_file_path = "nowcoding_rag_test.pdf"
    reader = PdfReader(pdf_file_path)
    text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

    # ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)
    docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]

    # Chroma DB ì„¤ì •
    db_path = "./nowcoding_DB"
    if os.path.exists(db_path):
        vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
        )
    else:
        vectorstore = Chroma.from_documents(
            docs,
            OpenAIEmbeddings(model="text-embedding-3-small"),
            persist_directory=db_path
        )
        vectorstore.persist()

    # RAG LLM ë° Retriever
    retriever = vectorstore.as_retriever()
    llm = ChatOpenAI(model="gpt-4.1-mini")

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
    ë„ˆëŠ” nowcoding ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.
    ì„¤ëª…ì„ ì…ë ¥ë°›ìœ¼ë©´ databaseì— ì €ì¥í•˜ê³  ì¶”í›„ì— ë¬¸ì œ í•´ê²°ì— í™œìš©í•œë‹¤.

    [ë¬¸ì„œ ë‚´ìš©]
    {context}

    [ëŒ€í™” ë‚´ìš© ë° ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """
    )

    # ë¬¸ì„œ í¬ë§· í•¨ìˆ˜
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # ğŸ”„ ëŒ€í™” ì €ì¥ í•¨ìˆ˜
    def store_user_input_to_chroma(user_input):
        conversation_text = f"user: {user_input}"
        doc = Document(page_content=conversation_text)
        vectorstore.add_documents([doc])
        vectorstore.persist()

    def store_ai_reply_to_chroma(assistant_reply):
        conversation_text = f"assistant: {assistant_reply}"
        doc = Document(page_content=conversation_text)
        vectorstore.add_documents([doc])
        vectorstore.persist()

    # RAG ì‹¤í–‰ í•¨ìˆ˜
    def run_RAG(user_input, chat_history):
        docs = retriever.invoke(user_input)
        context = format_docs(docs)
        conversation = '\n'.join(chat_history + [f'user: {user_input}'])
        prompt_text = prompt.format(context=context, question=conversation)
        response = llm.invoke(prompt_text)
        return response.content, docs

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ì´ì „ ëŒ€í™” ì¶œë ¥ (Chromaì— ì €ì¥ë˜ì—ˆì§€ë§Œ í™”ë©´ì—ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            if msg['role'] == 'assistant':
                st.code(msg['content'], language='python')
            else :
                st.text(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.text(user_input)

        try:
            # ì‘ë‹µ ìƒì„±
            response_text, source_docs = run_RAG(user_input, [f'{m["role"]}: {m["content"]}' for m in st.session_state.chat_history])

            # ì‘ë‹µ í‘œì‹œ
            with st.chat_message("assistant"):
                st.code(response_text, language='python')

                with st.expander("ğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš© ë³´ê¸°"):
                    for i, doc in enumerate(source_docs, 1):
                        st.text(f"**[{i}]** {doc.page_content.strip()[:500]}...")

            # ëŒ€í™” ì„¸ì…˜ ì €ì¥ (UIìš©)
            st.session_state.chat_history.append({"role": "user", "content": user_input})
            st.session_state.chat_history.append({"role": "assistant", "content": response_text})

            # âœ… ë§ˆì§€ë§‰ ëŒ€í™” ì„¸ì…˜ë„ ì €ì¥ (ì„ì‹œ ì €ì¥ìš©)
            st.session_state.last_user_input = user_input
            st.session_state.last_response = response_text

        except Exception as e:
            st.error(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # âœ… ëŒ€í™” ì €ì¥ ë²„íŠ¼ (ëŒ€í™” ìƒì„± ì´í›„ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥)
    if "last_user_input" in st.session_state and st.button("ğŸ’¾ ë§ˆì§€ë§‰ ì‚¬ìš©ì ì…ë ¥ì„ Chroma DBì— ì €ì¥í•˜ê¸°"):
        try:
            store_user_input_to_chroma(
                st.session_state.last_user_input
            )
            st.success("âœ… ë§ˆì§€ë§‰ ì‚¬ìš©ì ì…ë ¥ì„ Chroma DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âš ï¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if "last_response" in st.session_state and st.button("ğŸ’¾ ë§ˆì§€ë§‰ AI ì‘ë‹µì„ Chroma DBì— ì €ì¥í•˜ê¸°"):
        try:
            store_ai_reply_to_chroma(
                st.session_state.last_response
            )
            st.success("âœ… ë§ˆì§€ë§‰ AI ì‘ë‹µì„ Chroma DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âš ï¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ğŸ” ëŒ€í™” ë¦¬ì…‹ ë²„íŠ¼ (system ë©”ì‹œì§€ ì œì™¸)
    # ğŸ” ëŒ€í™” ë¦¬ì…‹ ë²„íŠ¼
    if st.button("âš ï¸ ëŒ€í™” ë¦¬ì…‹"):
        st.session_state.chat_history = []
        if "last_user_input" in st.session_state:
            del st.session_state.last_user_input
        if "last_response" in st.session_state:
            del st.session_state.last_response
        st.rerun()
