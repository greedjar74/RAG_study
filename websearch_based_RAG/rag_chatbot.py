import streamlit as st
import os
import sys
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ğŸ”§ ê²€ìƒ‰ + í¬ë¡¤ë§
def search_web(query, num_results=3, api_key=None):
    headers = {"X-API-KEY": api_key, "Content-Type": "application/json"}
    payload = {"q": query, "num": num_results}
    response = requests.post("https://google.serper.dev/search", json=payload, headers=headers)
    results = response.json().get("organic", [])
    return [item["link"] for item in results]

def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style"]): tag.decompose()
        return soup.get_text(separator="\n").strip()
    except Exception:
        return ""

# ğŸ”§ ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°í™” (Chroma ì œê±°)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

def get_search_docs(query, embedding_model, serper_key, k=5):
    urls = search_web(query, num_results=3, api_key=serper_key)
    docs = []
    for url in urls:
        text = extract_text_from_url(url)
        if text:
            chunks = text_splitter.split_text(text)
            docs.extend([Document(page_content=chunk, metadata={"source": url}) for chunk in chunks])

    if not docs:
        return None, []

    texts = [doc.page_content for doc in docs]
    embeddings = embedding_model.embed_documents(texts)

    def simple_retriever(query):
        query_vec = embedding_model.embed_query(query)
        sims = cosine_similarity([query_vec], embeddings)[0]
        top_indices = np.argsort(sims)[::-1][:k]
        return [docs[i] for i in top_indices]

    return simple_retriever, docs

# ğŸ”§ RAG ìˆ˜í–‰
def run_RAG(user_input, chat_history, serper_key):
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
    retriever_func, all_docs = get_search_docs(user_input, embedding_model, serper_key)
    if not retriever_func:
        return "ê²€ìƒ‰ëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", [], []

    docs = retriever_func(user_input)
    context = "\n\n".join(doc.page_content for doc in docs)
    conversation = "\n".join(chat_history + [f"user: {user_input}"])

    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""ë„ˆëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì•¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸ê³¼ ëŒ€í™” ë‚´ìš©]
{question}

[ë‹µë³€]
"""
    )
    prompt_text = prompt_template.format(context=context, question=conversation)

    llm = ChatOpenAI(model="gpt-4.1-mini")
    response = llm.invoke(prompt_text)

    return response.content, docs, [doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in docs]

# âœ… Streamlit UI
def rag_chatbot():
    st.title("ğŸ” ì‹¤ì‹œê°„ ê²€ìƒ‰ ê¸°ë°˜ RAG Chatbot")

    # ì‚¬ì´ë“œë°”
    st.sidebar.header("ğŸ” API ì„¤ì •")
    openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
    serper_key = st.sidebar.text_input("Serper.dev API Key", type="password")

    if not openai_key or not serper_key:
        st.warning("API í‚¤ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    os.environ["OPENAI_API_KEY"] = openai_key
    sys.stdout.reconfigure(encoding="utf-8")

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ì´ì „ ëŒ€í™” ì¶œë ¥
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                response_text, docs_used, sources = run_RAG(
                    user_input,
                    [f'{m["role"]}: {m["content"]}' for m in st.session_state.chat_history],
                    serper_key
                )

                with st.chat_message("assistant"):
                    st.markdown(response_text)

                    with st.expander("ğŸ“„ ì°¸ê³ í•œ ì›¹ ë¬¸ì„œ"):
                        for i, (doc, src) in enumerate(zip(docs_used, sources), 1):
                            preview = doc.page_content[:300].replace("\n", " ")
                            st.markdown(f"**[{i}]** [{src}]({src})\n\n`{preview}...`")

                st.session_state.chat_history.append({"role": "user", "content": user_input})
                st.session_state.chat_history.append({"role": "assistant", "content": response_text})

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if st.button("ğŸ” ëŒ€í™” ë¦¬ì…‹"):
        st.session_state.chat_history = []
        st.rerun()