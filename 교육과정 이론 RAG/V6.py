'''
# V6
- ëŒ€í™”í˜•
- ê¸°ì¡´ ëŒ€í™” ê¸°ì–µ
- DB ì˜êµ¬ ì €ì¥
- prompt í˜•íƒœ ì‚¬ìš©ìí™”
- ë¬¸ì„œì—ì„œ ì°¸ê³ í•œ ë‚´ìš©ì„ ì¶œë ¥
- rag_chain ëŒ€ì‹  ì§ì ‘ íŒŒì´ì¬ ì½”ë“œë¡œ rag êµ¬í˜„ -> ëŒ€í™” íˆìŠ¤í† ë¦¬ ë“± ì„¸ë¶€ì ì¸ ì„¤ì • ê°€ëŠ¥
'''

from langchain import hub
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
import sys
import os

# í™˜ê²½ ì„¤ì •
os.environ["OPENAI_API_KEY"] = ""  # ì‹¤ì œ í‚¤ë¡œ êµì²´í•˜ì„¸ìš”
sys.stdout.reconfigure(encoding='utf-8')

# PDF ë¶ˆëŸ¬ì˜¤ê¸° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
reader = PdfReader("summary of curriculum theory.pdf")
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)
docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]

# ì˜êµ¬ ë²¡í„° DB ìƒì„± ë° ì €ì¥
db_path = "./curriculum_theory_DB"

# DBê°€ ìˆëŠ” ê²½ìš°
if os.path.exists(db_path):
    vectorstore = Chroma(
        persist_directory=db_path,
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    ) # ê¸°ì¡´ DB ì‚¬ìš©

# DBê°€ ì—†ëŠ” ê²½ìš° -> DB ìƒì„±
else :
    vectorstore = Chroma.from_documents(
        docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=db_path
    )
    vectorstore.persist() # DB ìƒì„±

# retriever ì„¤ì •
retriever = vectorstore.as_retriever()

# LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4.1-mini")
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì„±ì‹¤í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""
)

# ë¬¸ì„œ ìŠ¤íƒ€ì¼ ë³€í™˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def run_RAG(user_input, chat_history):
    docs = retriever.invoke(user_input) # ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© íƒìƒ‰
    context = format_docs(docs) # ì°¸ê³  ìë£Œë¥¼ ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    conversation = '\n'.join(chat_history + [f'user: {user_input}']) # ê¸°ì¡´ ëŒ€í™” ë‚´ìš©, ì‚¬ìš©ì ì…ë ¥ì„ ë³‘í•©

    prompt_text = prompt.format(context=context, question=conversation) # í”„ë¡¬í”„íŠ¸ ìƒì„±

    response = llm.invoke(prompt_text) # ì§ˆë¬¸ì— ëŒ€í•œ ê²°ê³¼ ìƒì„±

    return response.content, docs # ê²°ê³¼ ë°˜í™˜

chat_history = []

# ëŒ€í™” ë£¨í”„
print("RAG ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)\n")
while True:
    user_input = input("ğŸ‘¤ ì§ˆë¬¸: ").strip()
    if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
        print("ğŸ›‘ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    try:
        response_text, source_docs = run_RAG(user_input, chat_history) # ê²°ê³¼ ë° ì°¸ê³  ìë£Œ ë°˜í™˜
        
        print("\nğŸ¤– ë‹µë³€:")
        for sentence in response_text.split('. '):
            print(sentence.strip())

        print('\nì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš© : ')
        for i, doc in enumerate(source_docs, 1):
            print(f'\n[{i}] {doc.page_content.strip()[:300]}...')
        print()  # ì¤„ë°”ê¿ˆ

    except Exception as e:
        print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)
    
    chat_history.append(f"user: {user_input}")
    chat_history.append(f"assistant: {response_text}")

    