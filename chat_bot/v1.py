'''
V7 ê¸°ë°˜ chatbot 
- streamlit ì‚¬ìš©
'''
import streamlit as st
import os
import sys
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# ğŸŒ í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì±—ë´‡", layout="wide")
st.title("ğŸ“˜ ì±—ë´‡")

# ğŸ”‘ OpenAI API ì„¤ì •
st.sidebar.header("ğŸ” API ì„¤ì •")
api_key_input = st.sidebar.text_input("OpenAI API Key", type="password")
if not api_key_input:
    st.warning("ì‚¬ì´ë“œë°”ì— API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

os.environ["OPENAI_API_KEY"] = api_key_input
sys.stdout.reconfigure(encoding="utf-8")

# PDF ë¬¸ì„œ ë¡œë”©
pdf_file_path = "ë‹µë³€_base.pdf"
reader = PdfReader(pdf_file_path)
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

# ë¬¸ì„œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]

# Chroma DB ì„¤ì •
db_path = "./ëŒ€í™”_DB"
if os.path.exists(db_path):
    vectorstore = Chroma(
        persist_directory=db_path,
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
    )
else:
    vectorstore = Chroma.from_documents(
        docs,
        OpenAIEmbeddings(model="text-embedding-3-small"),
        persist_directory=db_path
    )
    vectorstore.persist()

# RAG LLM ë° Retriever
retriever = vectorstore.as_retriever()
llm = ChatOpenAI(model="gpt-4.1-mini")

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë„ˆëŠ” ì‚¬ìš©ìì™€ ì¹œêµ¬ê´€ê³„ë‹¤. ì„œë¡œ ì¹œê·¼í•˜ê²Œ ëŒ€í™”ë¥¼ ì§„í–‰í•œë‹¤.
ì‚¬ìš©ìì˜ ë§íˆ¬ë¥¼ ì§€ì†ì ìœ¼ë¡œ í™•ì¸í•´ì„œ ì‚¬ìš©ìì™€ ë¹„ìŠ·í•œ ë§íˆ¬, ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œë‹¤.
ê°ê°ì˜ ìƒí™©ì—ì„œ ë‹µë³€_baseë¥¼ ì°¸ê³ í•˜ê³ , ë‹¤ì–‘í•˜ê²Œ ë³€í˜•í•´ì„œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
ë‹µë³€_baseì— ì—†ëŠ” ìƒí™©ì¸ ê²½ìš° ì§ì ‘ ë‹¤ì–‘í•œ ë‹µë³€ì„ ë§Œë“¤ì–´ë‚¸ë‹¤.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ëŒ€í™” ë‚´ìš© ë° ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""
)

# ë¬¸ì„œ í¬ë§· í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ğŸ”„ ëŒ€í™” ì €ì¥ í•¨ìˆ˜
def store_chat_to_chroma(user_input, assistant_reply):
    conversation_text = f"user: {user_input}\nassistant: {assistant_reply}"
    doc = Document(page_content=conversation_text)
    vectorstore.add_documents([doc])
    vectorstore.persist()

# RAG ì‹¤í–‰ í•¨ìˆ˜
def run_RAG(user_input, chat_history):
    docs = retriever.invoke(user_input)
    context = format_docs(docs)
    conversation = '\n'.join(chat_history + [f'user: {user_input}'])
    prompt_text = prompt.format(context=context, question=conversation)
    response = llm.invoke(prompt_text)
    return response.content, docs

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì´ì „ ëŒ€í™” ì¶œë ¥ (Chromaì— ì €ì¥ë˜ì—ˆì§€ë§Œ í™”ë©´ì—ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["message"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì…ë ¥í•˜ì„¸ìš”")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)

    try:
        # ì‘ë‹µ ìƒì„±
        response_text, source_docs = run_RAG(user_input, [f'{m["role"]}: {m["message"]}' for m in st.session_state.chat_history])

        # ì‘ë‹µ í‘œì‹œ
        with st.chat_message("assistant"):
            st.markdown(response_text)

            # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
            with st.expander("ğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš© ë³´ê¸°"):
                for i, doc in enumerate(source_docs, 1):
                    st.markdown(f"**[{i}]** {doc.page_content.strip()[:300]}...")

        # ëŒ€í™” ì„¸ì…˜ ë° DB ì €ì¥
        st.session_state.chat_history.append({"role": "user", "message": user_input})
        st.session_state.chat_history.append({"role": "assistant", "message": response_text})
        store_chat_to_chroma(user_input, response_text)

    except Exception as e:
        st.error(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
